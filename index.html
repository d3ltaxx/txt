<!DOCTYPE html>
<html>
  
<head>
    <style>
        body {
            margin: 100px;
        }
  
      
  
        button {
            width: 70px;
            height: 40px;
            margin-top: 120px;
            margin-left: 50px;
            background-color: green;
            color: white;
            border-radius: 10px;
            box-shadow: grey;
            position: absolute;
        }
  
        
    </style>
</head>
  
<body>
   
    <br />
      
    <!-- <button onclick="copyText()">1</button>
    <br />
    <br />
      
    <button onclick="copyText2()">2</button>
    <br />
    <br /> -->
      
    <button onclick="copyText3()">3</button>
    <br />
    <br />
      
    <button onclick="copyText4()">4</button>
    <br />
    <br />
      
    <button onclick="copyText5()">5</button>
    <br />
    <br />
      
    <button onclick="copyText6()">6</button>
    <br />
    <br />
      
    <button onclick="copyText7()">7</button>
    <br />
    <br />
      
    <button onclick="copyText8()">8</button>
    <br />
    <br />
      
    <button onclick="copyText9()">9</button>
    <br /> <br />
      
   

    
      
    <script>
        function copyText3() {
      
            /* Copy text into clipboard */
            navigator.clipboard.writeText
                (`sc = SparkContext()
rdd = sc.parallelize(["scala", "java", "hadoop", "spark", "akka", "spark vs hadoop",  
                                   â€œpyspark", "pyspark and spark"])
total_count = rdd.count()
spark_words_rdd = rdd.filter(lambda x: "spark" in x)
spark_words = spark_words_rdd.collect()
print(spark_words)`);
        }


        function copyText4() {
      
            /* Copy text into clipboard */
            navigator.clipboard.writeText
                (`pip install spark
Pip install pyspark
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc=SparkContext.getOrCreate()
spark=SparkSession(sc)   
rdd1=sc.parallelize([("spark", 1),("hadoop", 4)])
rdd2=sc.parallelize([("spark", 2),("hadoop", 5)])
rdd=sorted(rdd1.join(rdd2).collect())
Print(rdd)`);
        }


        function copyText5() {
      
            /* Copy text into clipboard */
            navigator.clipboard.writeText
                (`from pyspark import SparkContext
sc = SparkContext()
rdd = sc.parallelize([{1, 2, 3}, {4, 5, 6}, {7, 8, 9}])
acc = sc.accumulator(0)
def add_to_acc(x):
     global acc
     acc += sum(x)

rdd.foreach(add_to_acc)

print("Sum of numbers in RDD: ", acc.value)
`);
        }


function copyText6() {
      
            /* Copy text into clipboard */
            navigator.clipboard.writeText
                (`from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("CSV RDD").getOrCreate()

rdd = spark.read.format("csv").option("header", "true").load("/Book1.csv").rdd

print(rdd.take(5))
df = rdd.toDF()

df.describe().show()
`);
        }

        function copyText7() {
      
      /* Copy text into clipboard */
      navigator.clipboard.writeText
          (`from google.colab import files
uploaded = files.upload()
pip install pyspark
from pyspark.sql import SparkSession
spark=SparkSession.builder. appName("word_count"). getOrCreate()
text_file = spark.read.text("txtt.txt")
words = text_file.rdd.flatMap(lambda line: line.value.split("  "))
word_count = words.count()
print("Number of words in text file: ", word_count)`);
  }

  function copyText8() {
      
      /* Copy text into clipboard */
      navigator.clipboard.writeText
          (``);
  }

  function copyText9() {
      
      /* Copy text into clipboard */
      navigator.clipboard.writeText
          (`from pyspark.sql import SparkSession
from pyspark.sql.functions import col,sum,avg,max
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
Data = [("James","Sales","NY",90000,34,10000),
("Michael","Sales","NV",86000,56,20000),
("Robert","Sales","CA",81000,30,23000),
("Maria","Finance","CA",90000,24,23000),
("Raman","Finance","DE",99000,40,24000),
("Scott","Finance","NY",83000,36,19000),
("Jen","Finance","NY",79000,53,15000),
("Jeff","Marketing","NV",80000,25,18000),
("Kumar","Marketing","NJ",91000,50,21000)
]
schema = ["employee_name","department","state","salary","age","bonus"]
df = spark.createDataFrame(data=Data, schema = schema)
df.printSchema()
df.show(truncate=False)
df.groupBy("state").sum("salary").show()
df.groupBy("state").sum("salary").filter("sum(salary) > 100000").show()
df.groupBy("state").sum("salary").orderBy("sum(salary)", ascending=False).show()`);
  }

  function copyText10() {
      
      /* Copy text into clipboard */
      navigator.clipboard.writeText
          (``);
  }
    </script>
</body>
  
</html>
